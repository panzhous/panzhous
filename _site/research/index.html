<!DOCTYPE html>
<html lang="en">
  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      Research | Pan ZHOU (周攀)
    
  
</title>
<meta name="author" content="Pan ZHOU (周攀)">
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

  <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

<!-- Bootstrap Table -->


<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">



<!-- Styles -->

  <link rel="shortcut icon" href="/assets/img/log.ico?cada1a831dbcff16e2bae7854c63d30d">

<link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="http://localhost:4000/research/">

<!-- Dark Mode -->
<script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script>

  <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark">
  <script>
    initTheme();
  </script>


<!-- GeoJSON support via Leaflet -->


<!-- diff2html -->






  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/">
          Pan ZHOU (周攀)
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">Home
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                <li class="nav-item active">
                  
                  <a class="nav-link" href="/research/">Research
                    
                      <span class="sr-only">(current)</span>
                    
                  </a>
                </li>
              
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/publication/">Publication
                    
                  </a>
                </li>
              
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/teaching/">Teaching
                    
                  </a>
                </li>
              
            
          
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/services/">Services
                    
                  </a>
                </li>
              
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/recruitment/">Recruitment
                    
                  </a>
                </li>
              
            
          
          
            <!-- Toogle theme mode -->
            <li class="toggle-container">
              <button id="light-toggle" title="Change theme">
                <i class="ti ti-sun-moon" id="light-toggle-system"></i>
                <i class="ti ti-moon-filled" id="light-toggle-dark"></i>
                <i class="ti ti-sun-filled" id="light-toggle-light"></i>
              </button>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>


    <!-- Content -->
    <div class="container mt-5" role="main">
      
        <div class="post">
  <header class="post-header">
    <h1 class="post-title">Research</h1>
    <p class="post-description"></p>
  </header>

  <article>
    <div class="row">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/research7-480.webp 480w,/assets/img/research7-800.webp 800w,/assets/img/research7-1400.webp 1400w," sizes="95vw" type="image/webp"></source>
    
    <img src="/assets/img/research7.jpg" width="100%" height="auto" title="research image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

    </div>
</div>

<p>My research target is to build <strong>”efficient and effective artificial intelligent systems”</strong> so that machines can cognize, understand and interact with the environment. Currently, I mainly focus on three research topics across machine learning, computer vision and optimization.</p>

<p><strong>1)	Learning Framework</strong>: design effective learning framework / training task / loss to formulate a problem so that the AI models can learn desired knowledge to handle general / specific tasks.</p>

<ul>
  <li>
<strong>a) Self-Supervised (multi-modal) Learning</strong>: design effective and efficient self-supervised (multi-modal) learning frameworks that enable AI models to learn desired  knowledge and achieve human’s data understanding and reasoning ability.
    <ul>
      <li>
        <details> 
  <summary> <strong>Single-modal Learning</strong> (click here for representative works)</summary>

  <strong><a href="https://openreview.net/pdf?id=KmykpuSrjcq" rel="external nofollow noopener" target="_blank">PCL</a></strong> (ICLR, 800+ citations, 
  <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=salesforce&amp;repo=PCL&amp;type=star&amp;count=true">
  </iframe>) is the first clustering contrastive learning method to learn data cluster structure,  and its improved version, <strong><a href="https://arxiv.org/abs/2203.14415" rel="external nofollow noopener" target="_blank">Mugs</a></strong> (<iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=sail-sg&amp;repo=mugs&amp;type=star&amp;count=true">
  </iframe>), develops multi-granular contrastive learning for multi-granular representation learning. See more works like <strong><a href="https://arxiv.org/pdf/2106.14749.pdf" rel="external nofollow noopener" target="_blank">SANE</a></strong>(NeurIPS, spotlight), <strong><a href="https://arxiv.org/abs/2210.11016" rel="external nofollow noopener" target="_blank">TEC</a></strong>, and <strong><a href="https://arxiv.org/pdf/2106.09645.pdf" rel="external nofollow noopener" target="_blank">PGCL</a></strong> (TNNLS).
        

  </details>
      </li>
      <li>
        <details> 
  <summary> <strong>Multi-modal Learning</strong> (click here for representative works)</summary>

  <strong><a href="https://arxiv.org/pdf/2212.09737.pdf" rel="external nofollow noopener" target="_blank">PTP</a></strong> (CVPR &amp; TPAMI, 
  <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=sail-sg&amp;repo=ptp&amp;type=star&amp;count=true">
  </iframe>) is the pioneer to enhance grounding ability in multi-modal models, and LOVA<sup>3</sup> is to enpower models with humans' ability, including the answering, asking and accessing ability. See more works like <strong><a href="https://arxiv.org/abs/2312.02439" rel="external nofollow noopener" target="_blank">CLOT</a></strong>(CVPR, 
  <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=sail-sg&amp;repo=CLoT&amp;type=star&amp;count=true">
  </iframe>) for exploring humans' creativity, <strong><a href="https://arxiv.org/abs/2302.13668" rel="external nofollow noopener" target="_blank">CoVGT</a></strong>  (TPAMI &amp; ECCV, 
  <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=doc-doc&amp;repo=CoVGT&amp;type=star&amp;count=true">
  </iframe>) for video question answering,         <strong><a href="https://arxiv.org/abs/2312.06731" rel="external nofollow noopener" target="_blank">Genixer</a></strong> to empower multi-modal models as a powerful data generator,  <strong><a href="https://arxiv.org/pdf/2109.09161.pdf" rel="external nofollow noopener" target="_blank">Wav-BERT</a></strong> (AAAI) for acoustic and linguistic representation learning.

  </details>
      </li>
    </ul>
  </li>
  <li>
<strong>b) Generative Models</strong>: design  generative models like diffusion models that generate image/3D/video data and empower AI models with imagination and creativity akin to that of humans.
    <ul>
      <li>
        <details> 
<summary> <strong>Image Generation</strong> (click here for representative works)</summary>

<strong><a href="https://arxiv.org/abs/2303.14389" rel="external nofollow noopener" target="_blank">MDT</a></strong> (ICCV,
<iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="400px" height="20px" src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/masked-diffusion-transformer-is-a-strong/image-generation-on-imagenet-256x256">
</iframe>,
<iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=sail-sg&amp;repo=MDT&amp;type=star&amp;count=true">
</iframe>) achieves SoTA image synthesis performance on ImageNet (256x256), and improves the learning speed of <a href="https://arxiv.org/abs/2212.09748" rel="external nofollow noopener" target="_blank">DiT</a> (
<iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=facebookresearch&amp;repo=DiT&amp;type=star&amp;count=true">
</iframe>), the core component of <a href="https://openai.com/sora" rel="external nofollow noopener" target="_blank">SORA</a>, by at least 10x.  See more works like <strong><a href="https://github.com/sail-sg/EditAnything" rel="external nofollow noopener" target="_blank">EditAnything</a></strong>  (
<iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=sail-sg&amp;repo=EditAnything&amp;type=star&amp;count=true">
</iframe>), <strong><a href="https://arxiv.org/abs/2306.06991" rel="external nofollow noopener" target="_blank">FDT</a></strong> (
<iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=sail-sg&amp;repo=FDM&amp;type=star&amp;count=true">
</iframe>), <strong><a href="https://arxiv.org/pdf/2310.13545.pdf" rel="external nofollow noopener" target="_blank">ScaleLong</a></strong>  (NeurIPS, 
<iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=sail-sg&amp;repo=ScaleLong&amp;type=star&amp;count=true">
</iframe>), and <strong><a href="https://arxiv.org/pdf/2006.06900.pdf" rel="external nofollow noopener" target="_blank">PPOGAN</a></strong>  (NeurIPS, 
<iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=Holmeswww&amp;repo=PPOGAN&amp;type=star&amp;count=true">
</iframe>).
</details>
      </li>
      <li>
        <details> 
  <summary> <strong>3D Generation</strong> (click here for representative works)</summary>

  <strong><a href="https://arxiv.org/abs/2401.09050" rel="external nofollow noopener" target="_blank">Consistent3D</a></strong> (CVPR, 
  <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=sail-sg&amp;repo=Consistent3D&amp;type=star&amp;count=true">
  </iframe>) is the pioneer to use ODE sampling as guidance in text-to-3D task, overcoming the unpredicable and unstable SDE guidance in  <a href="https://arxiv.org/abs/2209.14988" rel="external nofollow noopener" target="_blank">SDS/DreamFusion</a>. See more works, e.g., <strong><a href="ttps://github.com/yxymessi/DTC123/blob/main/DTC_CVPR.pdf">DTC123</a></strong> (CVPR, 
  <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=yxymessi&amp;repo=DTC123&amp;type=star&amp;count=true">
  </iframe>) and <strong><a href="https://arxiv.org/abs/2403.18795" rel="external nofollow noopener" target="_blank">Gamba</a></strong>  for image-to-3D generation, <strong><a href="https://arxiv.org/abs/2311.08403" rel="external nofollow noopener" target="_blank">Instant3D</a></strong> (
  <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=ming1993li&amp;repo=Instant3DCodes&amp;type=star&amp;count=true">
  </iframe>) for fast text-to-3D task.


  </details>
      </li>
    </ul>
  </li>
</ul>

<p><!-- * **c) Meta In-Context Learning**: design new meta-learning and prompt learning methods to aid a (pretrained) model in quickly learning from a few data, improving few-shot learning ability of AGI.  --></p>

<p><strong>2)	Network Architecture Design</strong>: develop innovative network topology that posses high capacity and efficiency for acquiring knowledge, thereby improving the overall model capacity of AI.</p>
<ul>
  <li>
    <details> 
  <summary> <strong>Manually-Designed Network</strong> (click here for representative works)</summary>

  <strong><a href="https://arxiv.org/abs/2111.11418#:~:text=Based%20on%20the%20extensive%20experiments,on%20the%20token%20mixer%20modules." rel="external nofollow noopener" target="_blank">MetaFormer</a></strong> (CVPR ORAL, 600+ citations, 
  <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=sail-sg&amp;repo=poolformer&amp;type=star&amp;count=true">
  </iframe>) replaces self-attention in ViT with pooling and convolutions independently,  and achieves impressive performance, breaking the slogan “self-attention is all you need”. It reveals network design princeples that if a network contains two kinds of operations, including  spatial information exchanging operations (e.g., attention, pooling and convolution) and  channel information exchanging operations  (e.g., MLP), then the network can perfor well.  Its improved version CAFormer network sets a new recording accuracy of 85.5% on ImageNet under supervised settings without extra training data, and achives top-2 performance on ImageNet-C( 
  <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="400px" height="20px" src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/metaformer-baselines-for-vision/domain-generalization-on-imagenet-c">
  </iframe>). See more works like <strong><a href="https://arxiv.org/abs/2205.12956" rel="external nofollow noopener" target="_blank">IFormer</a></strong>  (
  <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=sail-sg&amp;repo=iFormer&amp;type=star&amp;count=true">
  </iframe>), <strong><a href="https://arxiv.org/abs/2303.16900" rel="external nofollow noopener" target="_blank">InceptionNeXt</a></strong>  (
  <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=sail-sg&amp;repo=inceptionnext&amp;type=star&amp;count=true">
  </iframe>), <strong><a href="https://arxiv.org/abs/2112.04674" rel="external nofollow noopener" target="_blank">DualFormer</a></strong> (
  <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=sail-sg&amp;repo=dualformer&amp;type=star&amp;count=true">
  </iframe>), and <strong><a href="https://arxiv.org/abs/2203.07057" rel="external nofollow noopener" target="_blank">SUN</a></strong>  (ECCV, 
  <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=DongSky&amp;repo=few-shot-vit&amp;type=star&amp;count=true">
  </iframe>).

  </details>
  </li>
  <li>
    <details> 
  <summary> <strong>Automatically-Designed Network</strong> (click here for representative works)</summary>

  <strong><a href="https://arxiv.org/abs/2006.16537" rel="external nofollow noopener" target="_blank">PR-DARTS</a></strong> (NeurIPS ORAL, 
  <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=salesforce&amp;repo=PR-DARTS&amp;type=star&amp;count=true">
  </iframe>)  automatically designs effective network architectures, reducing the reliance on expert trial and error. It provides the first theory to show why previous network search methods (a.k.a. AutoML) often collapse due to selecting too many skip-connections, and then proposes a new method that can avoid previous collapse and thus automatically selects and combines various network operations, e.g. pooling and convolution, to search more effective network.
    
  </details>
  </li>
</ul>

<p><strong>3)	Parameter Optimizer</strong>: design efficient optimizers to train AI models efficiently.</p>
<ul>
  <li>
    <details> 
  <summary> <strong>Faster Optimizer</strong> (click here for representative works)</summary>

  <strong><a href="https://arxiv.org/abs/2208.06677" rel="external nofollow noopener" target="_blank">Adan</a></strong> (
  <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=sail-sg&amp;repo=Adan&amp;type=star&amp;count=true">
  </iframe>) is about 2x faster than the SoTA optimizers, e.g. Adam, while achieving higher or comparable performance on many networks, e.g., CNNs, ViTs and MAE in the CV field, UNet and ViTs in AIGC field, GPT2 and billion-scale LLaMA in the NLP field, networks in RL tasks. It has been included in popular deep-learning codebases, e.g.,  <a href="https://github.com/NVIDIA/NeMo/blob/main/nemo/core/optim/adan.py" rel="external nofollow noopener" target="_blank">NVIDIA NeMo</a> (
  <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=NVIDIA&amp;repo=NeMo&amp;type=star&amp;count=true">
  </iframe>) for training large language models and multi-modal models,  <a href="https://github.com/huggingface/pytorch-image-models/blob/main/timm/optim/adan.py" rel="external nofollow noopener" target="_blank">HuggingFace Timm</a> (
  <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=huggingface&amp;repo=pytorch-image-models&amp;type=star&amp;count=true">
  </iframe>) and <a href="https://github.com/open-mmlab/mmpretrain/blob/dev-1.x/mmcls/engine/optimizers/adan_t.py" rel="external nofollow noopener" target="_blank">OpenMMLab</a> (
  <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=open-mmlab&amp;repo=mmpretrain&amp;type=star&amp;count=true">
  </iframe>) which both train AI models for CV tasks like classification, detection and segmentation, <a href="https://github.com/Jittor/jittor/blob/master/python/jittor/optim.py" rel="external nofollow noopener" target="_blank">Jittor of Tsinghua University</a> (
  <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=Jittor&amp;repo=jittor&amp;type=star&amp;count=true">
  </iframe>) for 3D generation, and is the default optimizer in <a href="https://github.com/ashawkey/stable-dreamfusion/blob/main/optimizer.py" rel="external nofollow noopener" target="_blank">DreamFusion</a> (
  <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=ashawkey&amp;repo=stable-dreamfusion&amp;type=star&amp;count=true">
  </iframe>) and <a href="https://arxiv.org/abs/2303.14389" rel="external nofollow noopener" target="_blank">MDT</a>  (
  <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=sail-sg&amp;repo=MDT&amp;type=star&amp;count=true">
  </iframe>) for SoTA 3D and image generation tasks respectively.
    
    
  <p>See more works, e.g., <strong><a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf" rel="external nofollow noopener" target="_blank">SLRLA</a></strong> (NeurIPS, 
  <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=sail-sg&amp;repo=SLRLA-optimizer&amp;type=star&amp;count=true">
  </iframe>) which improves lookahead,  <strong><a href="https://ieeexplore.ieee.org/document/8792163" rel="external nofollow noopener" target="_blank">R-SPIDER</a></strong> (TPAMI &amp; AISTATS), and  <strong><a href="https://arxiv.org/pdf/2009.09835.pdf" rel="external nofollow noopener" target="_blank">HSDMPG</a></strong> (ICML &amp; TPAMI). </p>
    
  </details>
  </li>
  <li>
    <details> 
  <summary> <strong>Unified Plug-and-play Accelration Framework</strong> (click here for representative works)</summary>

  <strong><a href="../assets/pdf/2024-JMLR-win.pdf">Win</a></strong> (JMLR &amp; ICLR, 
  <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=sail-sg&amp;repo=win&amp;type=star&amp;count=true">
  </iframe>)  can accelerate AdamW, Adam, LAMB and SGD by 1.5X on vision classification tasks and language modeling tasks with both CNN and Transformer backbones.   
    
  </details>
  </li>
  <li>
    <details> 
  <summary> <strong>Network Optimization Theory</strong> (click here for representative works)</summary>

  <strong><a href="https://arxiv.org/pdf/2010.05627.pdf" rel="external nofollow noopener" target="_blank">This work</a></strong> (NeurIPS, 200+ citations
  <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=salesforce&amp;repo=comparison_SGD_ADAM&amp;type=star&amp;count=true">
  </iframe>) provides the first theory to explain  "why SGD generalizes better than ADAM in deep learning".  See more works like <strong><a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf" rel="external nofollow noopener" target="_blank">SLRLA</a></strong> (NeurIPS, 
  <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=sail-sg&amp;repo=SLRLA-optimizer&amp;type=star&amp;count=true">
  </iframe>) to analyzes  "why Lookahead generalizes better than SGD". 
    
  </details>
  </li>
</ul>


  </article>

  

  
</div>

      
    </div>

    <!-- Footer -->
    
  <footer class="fixed-bottom" role="contentinfo">
    <div class="container mt-0">
      © Copyright 2024
      Pan
      
      ZHOU (周攀). 
      
      
    </div>
  </footer>



    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
<script src="/assets/js/bootstrap.bundle.min.js"></script>
<!-- <script src="/assets/js/mdb.min.js"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
  <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>


    

    

    

    

    

    

    

    

  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>



<!-- Bootstrap Table -->


<!-- Load Common JS -->
<script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script>
<script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script>
<script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script>

<!-- Jupyter Open External Links New Tab -->
<script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script>



    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>


  <script async src="https://badge.dimensions.ai/badge.js"></script>


    
  <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams',
      },
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>


    


    
  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    /*
     * This JavaScript code has been adapted from the article
     * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
     * published on the website https://css-tricks.com on the 7th of May, 2014.
     * Couple of changes were made to the original code to make it compatible
     * with the `al-foio` theme.
     */
    const progressBar = $('#progress');
    /*
     * We set up the bar after all elements are done loading.
     * In some cases, if the images in the page are larger than the intended
     * size they'll have on the page, they'll be resized via CSS to accomodate
     * the desired size. This mistake, however, breaks the computations as the
     * scroll size is computed as soon as the elements finish loading.
     * To account for this, a minimal delay was introduced before computing the
     * values.
     */
    window.onload = function () {
      setTimeout(progressBarSetup, 50);
    };
    /*
     * We set up the bar according to the browser.
     * If the browser supports the progress element we use that.
     * Otherwise, we resize the bar thru CSS styling
     */
    function progressBarSetup() {
      if ('max' in document.createElement('progress')) {
        initializeProgressElement();
        $(document).on('scroll', function () {
          progressBar.attr({ value: getCurrentScrollPosition() });
        });
        $(window).on('resize', initializeProgressElement);
      } else {
        resizeProgressBar();
        $(document).on('scroll', resizeProgressBar);
        $(window).on('resize', resizeProgressBar);
      }
    }
    /*
     * The vertical scroll position is the same as the number of pixels that
     * are hidden from view above the scrollable area. Thus, a value > 0 is
     * how much the user has scrolled from the top
     */
    function getCurrentScrollPosition() {
      return $(window).scrollTop();
    }

    function initializeProgressElement() {
      let navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      progressBar.css({ top: navbarHeight });
      progressBar.attr({
        max: getDistanceToScroll(),
        value: getCurrentScrollPosition(),
      });
    }
    /*
     * The offset between the html document height and the browser viewport
     * height will be greater than zero if vertical scroll is possible.
     * This is the distance the user can scroll
     */
    function getDistanceToScroll() {
      return $(document).height() - $(window).height();
    }

    function resizeProgressBar() {
      progressBar.css({ width: getWidthPercentage() + '%' });
    }
    // The scroll ratio equals the percentage to resize the bar
    function getWidthPercentage() {
      return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
    }
  </script>


    

    

  </body>
</html>
